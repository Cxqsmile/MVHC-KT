{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from scipy.sparse import dok_matrix,lil_matrix, save_npz\n",
    "import torch \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import load_npz\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYS = [\"Anon Student Id\", \"KC(Default)\", \"Questions\"]\n",
    "read_trainfile = \"raw/algebra_2005_2006_train.txt\"\n",
    "read_testfile = \"raw/algebra_2005_2006_test.txt\"\n",
    "#write_file = \"raw/algebra_05_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text(text):\n",
    "    text = text.replace(\"_\", \"####\").replace(\",\", \"@@@@\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_table(read_trainfile, encoding = \"utf-8\", low_memory=False)\n",
    "df_train[\"Problem Name\"] = df_train[\"Problem Name\"].apply(replace_text)\n",
    "df_train[\"Step Name\"] = df_train[\"Step Name\"].apply(replace_text)\n",
    "#df_train[\"Questions\"] = df_train.apply(lambda x:f\"{x['Problem Name']}----{x['Step Name']}\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_table(read_testfile, encoding = \"utf-8\", low_memory=False)\n",
    "df_test[\"Problem Name\"] = df_test[\"Problem Name\"].apply(replace_text)\n",
    "df_test[\"Step Name\"] = df_test[\"Step Name\"].apply(replace_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape) \n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.shape) \n",
    "print(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.concat([df_train, df_test], axis=0)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "output_file = \"raw/algebra_05_data_all.csv\"\n",
    "data.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"The data has been successfully saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our algebra05 dataset we have First Transaction Time = order_id\n",
    "data = pd.read_csv(\n",
    "    'raw/algebra_05_data_all.csv',\n",
    "    usecols=['Anon Student Id', 'Problem Name', 'KC(Default)', 'First Transaction Time','Correct First Attempt']\n",
    ").dropna(subset=['Anon Student Id', 'Problem Name', 'KC(Default)', 'First Transaction Time','Correct First Attempt']) \n",
    "\n",
    "# 改名 \n",
    "data = data.rename(columns={'Anon Student Id': 'user_id'})\n",
    "data = data.rename(columns={'Problem Name': 'problem_id'})\n",
    "data = data.rename(columns={'KC(Default)': 'skill_id'})\n",
    "data = data.rename(columns={'First Transaction Time': 'order_id'})\n",
    "data = data.rename(columns={'Correct First Attempt': 'correct'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['order_id', 'user_id', 'problem_id', 'skill_id', 'correct'])  \n",
    "data = data[data[\"correct\"].isin([0, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time format originally: 2005/9/9 12:24:49, changed to digital milliseconds\n",
    "# Improved function to support multiple date formats\n",
    "def change2timestamp(t, date_format=\"%Y-%m-%d %H:%M:%S.%f\"):\n",
    "    timeStamp = datetime.strptime(t, date_format).timestamp() * 1000\n",
    "    return int(timeStamp)\n",
    "\n",
    "# Convert order_id to millisecond timestamp\n",
    "data[\"order_id\"] = data[\"order_id\"].apply(lambda x: change2timestamp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['correct']= data['correct'].astype(int) #important！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean up and standardize skill names\n",
    "def standardize_skill(skill):\n",
    "    return skill.strip()\n",
    "\n",
    "# Extract all individual skills and sub-skills\n",
    "all_single_skills = set()\n",
    "for skill in data['skill_id']:\n",
    "    components = skill.split('~~')  # Split and Compose skills\n",
    "    all_single_skills.update(standardize_skill(comp) for comp in components)\n",
    "\n",
    "# Initialize mappings\n",
    "skill_mapping = {}\n",
    "inverse_mapping = {}\n",
    "counter = 0\n",
    "\n",
    "# Deal with individual skills first\n",
    "for single_skill in sorted(all_single_skills): # sort makes sure the numbers are in the same order\n",
    "    skill_mapping[single_skill] = counter\n",
    "    inverse_mapping[counter] = [counter]  \n",
    "    counter += 1\n",
    "\n",
    "# Reprocessing composition skills\n",
    "for skill in data['skill_id']:\n",
    "    skill = standardize_skill(skill)\n",
    "    if '~~' in skill:\n",
    "        components = skill.split('~~')\n",
    "        components = [standardize_skill(comp) for comp in components]\n",
    "        components = sorted(components)  #Sort to ensure consistency in the order of the same combination\n",
    "        combined_key = '~~'.join(components)  #Recombine into standard form\n",
    "        if combined_key not in skill_mapping:\n",
    "            component_ids = [skill_mapping[comp] for comp in components]  #Mapping sub skills to numbers\n",
    "            skill_mapping[combined_key] = counter\n",
    "            inverse_mapping[counter] = component_ids  #Combination of record numbering forms\n",
    "            counter += 1\n",
    "\n",
    "#Map skill numbers to data\n",
    "def map_skill_to_id(skill):\n",
    "    if '~~' in skill:\n",
    "        components = skill.split('~~')\n",
    "        components = sorted(standardize_skill(comp) for comp in components)\n",
    "        combined_key = '~~'.join(components)\n",
    "        if combined_key in skill_mapping:\n",
    "            return skill_mapping[combined_key]\n",
    "        else:\n",
    "            raise KeyError(f\"The combination skill {combined_key} was not found in skill_mapping.\")\n",
    "    else:\n",
    "        single_key = standardize_skill(skill)\n",
    "        if single_key in skill_mapping:\n",
    "            return skill_mapping[single_key]\n",
    "        else:\n",
    "            raise KeyError(f\"The single skill {single_key} was not found in skill_mapping.\")\n",
    "\n",
    "#Application Mapping\n",
    "data['skill_id_mapped'] = data['skill_id'].apply(map_skill_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skill Number Mapping (First 3)：\")\n",
    "print(dict(list(skill_mapping.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverte_mapping records which sub concepts make up a composite concept, which is then used to generate a matrix\n",
    "print(\"\\nReverse mapping (which small concepts make up a new concept):\")  \n",
    "print(dict(list(inverse_mapping.items())))\n",
    "\n",
    "print(\"\\nMapped data (first 3 lines):\")\n",
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the processed data as a CSV file according to the requirements\n",
    "output_file = \"raw/algebra_05_data.csv\"\n",
    "data.to_csv(output_file, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the maximum value of the skill_id_mapped column\n",
    "max_value = data['skill_id_mapped'].max()\n",
    "#Output maximum value\n",
    "print(f\"The maximum value of the skill_id_mapped column is:{max_value}\")\n",
    "\n",
    "#This records the mapping relationship of all concepts, and in order, the matrix size of the second channel should be determined by this\n",
    "print(f\"The length of inverte_mapping is:{len(inverse_mapping)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_values = sorted(data['skill_id_mapped'].unique())\n",
    "\n",
    "#Check if it is arranged in order\n",
    "is_sequential = mapped_values == list(range(mapped_values[0], mapped_values[-1] + 1))\n",
    "\n",
    "#Output result\n",
    "print(f\"skill_id_mapped column sorted values:：{mapped_values}\")\n",
    "print(f\"Is the skill_id_mapped column arranged in order：{'yes' if is_sequential else 'no'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_len = None\n",
    "problems_len = None\n",
    "#View the data situation after removing the required columns and leaving them empty\n",
    "def dataShow():\n",
    "    global skills_len  \n",
    "    global problems_len\n",
    "    print(data.shape)\n",
    "\n",
    "    users=data.user_id.unique().tolist() \n",
    "    print('Total number of students：',len(users))\n",
    "\n",
    "    problem_list = data.problem_id.unique().tolist()\n",
    "    print('Total number of questions：',len(problem_list))\n",
    "    problems_len = len(problem_list)\n",
    "\n",
    "    #skill_list = data.skill_id.unique().tolist()\n",
    "    print('Total number of skills：',len(inverse_mapping))#技能数要看概念映射关系inverse_mapping的总长度了\n",
    "    skills_len = len(inverse_mapping)\n",
    "\n",
    "dataShow()\n",
    "print(skills_len)\n",
    "print(problems_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal data numbering changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.groupby('user_id').filter(lambda x:len(x)>2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataShow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish a practice mapping, numbered from 0 to n (this way, after establishing the correlation matrix, it will not be too large, and questions can be found directly according to the index)\n",
    "raw_problem=data.problem_id.unique().tolist()\n",
    "raw_problem.sort()  \n",
    "num_problem=len(raw_problem)\n",
    "problems={p:i for i,p in enumerate(raw_problem)}\n",
    "print(\"number of problems: %d\" % num_problem)\n",
    "\n",
    "print(type(problems))\n",
    "#Output the first 5 key value pairs of the dictionary\n",
    "def print_first_five_items(data):\n",
    "    for i, (key, value) in enumerate(data.items()):\n",
    "        print(f'{key}: {value}')\n",
    "        if i >= 4:  \n",
    "            break\n",
    "print_first_five_items(problems)\n",
    "\n",
    "#Replace problem_id with the value of qMap\n",
    "data['problem_id']=data['problem_id'].map(problems)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the assist09 and the assist17 dataset\n",
    "# ##Establish skill mapping, numbered from 0 to n\n",
    "# raw_question = data.skill_id.unique().tolist()  \n",
    "# num_skill = len(raw_question)\n",
    "# skills = { p: i for i, p in enumerate(raw_question) }  \n",
    "# print(\"number of skills: %d\" % num_skill)\n",
    "# #np.save('data/'+dataset+'/map/cMap.npy',skills)\n",
    "\n",
    "# print(type(skills))\n",
    "\n",
    "# ##Output the first 5 key value pairs of the dictionary\n",
    "# def print_first_five_items(data):\n",
    "#     for i, (key, value) in enumerate(data.items()):\n",
    "#         print(f'{key}: {value}')\n",
    "#         if i >= 4:\n",
    "#             break\n",
    "\n",
    "# print_first_five_items(skills)\n",
    "\n",
    "# data['skill_id']=data['skill_id'].map(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish student mapping, numbered from 0 to n (easy to find students after grouping by student column, find students by index number+1)\n",
    "raw_student = data.user_id.unique().tolist()  \n",
    "num_student = len(raw_student)\n",
    "students = { s: i for i, s in enumerate(raw_student) } \n",
    "print(\"number of students: %d\" % num_student)\n",
    "#np.save('data/'+dataset+'/map/sMap.npy',students)\n",
    "\n",
    "print(type(students))\n",
    "def print_first_five_items(data):\n",
    "    for i, (key, value) in enumerate(data.items()):\n",
    "        print(f'{key}: {value}')\n",
    "        if i >= 4: \n",
    "            break\n",
    "print_first_five_items(students)\n",
    "\n",
    "data['user_id']=data['user_id'].map(students)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data into the prediction network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide each student's answer sequence into blocks\n",
    "def parse_all_seq(students):\n",
    "    all_sequences = []\n",
    "    split_count = 0  #Record the number of splits\n",
    "    for student_id in tqdm.tqdm(students, 'parse student sequence:\\t'):\n",
    "        student_sequence = parse_student_seq(data[data.user_id == student_id])\n",
    "        \n",
    "        problem_ids, skill_ids, corrects = student_sequence\n",
    "        if len(problem_ids) > 200:\n",
    "            # Sort by order_id and split into multiple sequences, with each segment not exceeding 200\n",
    "            student_sorted = data[data.user_id == student_id].sort_values('order_id')\n",
    "            problem_ids_sorted = student_sorted['problem_id'].values\n",
    "            #skill_ids_sorted = student_sorted['skill_id'].values\n",
    "            skill_ids_sorted = student_sorted['skill_id_mapped'].values\n",
    "            corrects_sorted = student_sorted['correct'].values\n",
    "            \n",
    "            for i in range(0, len(problem_ids_sorted), 200):\n",
    "                sliced_sequence = (\n",
    "                    problem_ids_sorted[i:i+200],\n",
    "                    skill_ids_sorted[i:i+200],\n",
    "                    corrects_sorted[i:i+200]\n",
    "                )\n",
    "                \n",
    "                 #Determine if the length of the segmented sequence is less than 3. If it is less than 3, discard it\n",
    "                if len(sliced_sequence[0]) < 3:\n",
    "                    continue  #Skip the current sequence\n",
    "                \n",
    "                all_sequences.append(sliced_sequence)\n",
    "                split_count += 1  #Increase the count every time it is split\n",
    "        else:\n",
    "            #If the length is less than or equal to 200, add directly\n",
    "            if len(problem_ids) >= 3: \n",
    "                all_sequences.append(student_sequence)\n",
    "            \n",
    "        #all_sequences.extend([student_sequence])\n",
    "    return all_sequences, split_count\n",
    "\n",
    "def parse_student_seq(student):\n",
    "    seq = student.sort_values('order_id')\n",
    "    return seq['problem_id'].values,seq['skill_id_mapped'].values, seq['correct'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, split_count = parse_all_seq(sorted(data.user_id.unique()))\n",
    "print(f\"Number of student sequence splits: {split_count}\") ##This is the number of splits, not a total of a few more students. For example, a sequence of 400 will be split twice, but compared to the original, it only has one more sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sequences))\n",
    "n = 5\n",
    "print(sequences[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of students：',len(sequences))\n",
    "print(sequences[0][0])   \n",
    "print(len(sequences[0][0])) #seq['problem_id'].values\n",
    "print(len(sequences[0][1])) #seq['skill_id_mapped'].values\n",
    "print(len(sequences[0][2])) #seq['correct'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the length of each sequence and find the maximum length\n",
    "max_length = max(len(seq[0]) for seq in sequences) \n",
    "\n",
    "print(f\"The maximum sequence length is: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences, dtype=object)\n",
    "print(sequences.shape)\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_empty = any(\n",
    "    seq is None or any(len(part) == 0 for part in seq) for seq in sequences\n",
    ")\n",
    "\n",
    "if has_empty:\n",
    "    print(\"There are empty values or partially empty sequences in the sequences.\")\n",
    "else:\n",
    "    print(\"There are no empty values in sequences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(27)\n",
    "np.random.shuffle(sequences)\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "data_size=len(sequences)\n",
    "\n",
    "train_end = int(train_ratio * data_size)\n",
    "val_end = int((train_ratio + val_ratio) * data_size)\n",
    "train_set,valid_set,test_set=np.split(sequences,[train_end,val_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.shape,valid_set.shape,test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('lstm_data/train_set.npy',train_set)\n",
    "np.save('lstm_data/valid_set.npy',valid_set)\n",
    "np.save('lstm_data/test_set.npy',test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing with the preparation of data for each channel below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert format to convert problems and concepts into key value pairs\n",
    "def problemid_skillid_To_key_value(data):\n",
    "    result = {}   \n",
    "    count_dict = {}  \n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        problem_id = row['problem_id']\n",
    "        #skill_id = row['skill_id']\n",
    "        skill_id = row['skill_id_mapped']\n",
    "        \n",
    "        if pd.notna(skill_id) and skill_id != '':\n",
    "            #If problem_id is already in the dictionary, add skill_id to the collection\n",
    "            if problem_id in result:\n",
    "                result[problem_id].add(skill_id)\n",
    "            else:\n",
    "               #If problem_id is not in the dictionary, create a new set and add skill_id\n",
    "                result[problem_id] = {skill_id}\n",
    "    \n",
    "    result = {k: list(v) for k, v in result.items()}\n",
    "    count_dict = {k: len(v) for k, v in result.items()}\n",
    "    \n",
    "    return result, count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemid_skillid_key_value,problemid_skillid_key_value_count_dict = problemid_skillid_To_key_value(data)\n",
    "\n",
    "problemid_skillid_key_value = dict(sorted(problemid_skillid_key_value.items()))\n",
    "problemid_skillid_key_value_count_dict = dict(sorted(problemid_skillid_key_value_count_dict.items()))\n",
    "\n",
    "my_dict = problemid_skillid_key_value  #Problemid_stkillid_key-value: The format of the key value pairs between the problem and the concepts it contains\n",
    "n = 10 \n",
    "first_n_pairs = {k: my_dict[k] for k in list(my_dict)[:n]}\n",
    "print(first_n_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_reverse_traj(users_trajs_dict):\n",
    "    \"\"\"Get each user's reversed trajectory according to her complete trajectory\"\"\"\n",
    "    users_rev_trajs_dict = {}\n",
    "    for userID, traj in users_trajs_dict.items():\n",
    "        rev_traj = traj[::-1]\n",
    "        users_rev_trajs_dict[userID] = rev_traj\n",
    "\n",
    "    return users_rev_trajs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemid_skillid_rev_key_value = get_user_reverse_traj(problemid_skillid_key_value)  \n",
    "\n",
    "my_dict = problemid_skillid_rev_key_value\n",
    "n = 10  \n",
    "\n",
    "first_n_pairs = {k: my_dict[k] for k in list(my_dict)[:n]}\n",
    "print(first_n_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The third channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_len = skills_len  \n",
    "print(skills_len)\n",
    "\n",
    "# 1.  Extract all possible skill_ids as dimensions\n",
    "all_skills = sorted({skill for skills in problemid_skillid_key_value.values() for skill in skills})\n",
    "print(len(all_skills))\n",
    "\n",
    "# 2.  Obtain the total number of questions\n",
    "num_problems = problems_len\n",
    "print(num_problems)\n",
    "\n",
    "# 3.  Initialize NumPy two-dimensional array, with each row corresponding to a problem vector\n",
    "problem_vectors = np.zeros((num_problems, skills_len), dtype=int)\n",
    "\n",
    "# 4.  Convert each problem_id into a vector and store it in a two-dimensional array\n",
    "for idx, (problem_id, skills) in enumerate(problemid_skillid_key_value.items()):\n",
    "    for skill in skills:  #For each question, set the position corresponding to the included skill_id to 1\n",
    "        problem_vectors[idx, all_skills.index(skill)] = 1\n",
    "\n",
    "# 5. Output the vector corresponding to each question\n",
    "# for idx, (problem_id, _) in enumerate(problemid_skillid_key_value.items()):\n",
    "    # print(f\"Problem {problem_id}: {problem_vectors[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem-vectors calculates cosine similarity\n",
    "num_questions = problems_len\n",
    "vector_dim = skills_len\n",
    "vectors = problem_vectors\n",
    "\n",
    "#Initialize sparse matrix for storing cosine similarity\n",
    "similarity_matrix = lil_matrix((num_questions, num_questions))\n",
    "\n",
    "#Calculate the cosine similarity of the upper triangle part one by one\n",
    "for i in range(num_questions):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Processing question {i}/{num_questions}\")  \n",
    "    \n",
    "    #Calculate the cosine similarity between problem i and all subsequent problems\n",
    "    if i + 1 < num_questions:  #Ensure that no empty arrays are passed\n",
    "        similarities = cosine_similarity(vectors[i].reshape(1, -1), vectors[i+1:])[0]\n",
    "\n",
    "        for j, sim in enumerate(similarities, start=i+1):\n",
    "            if sim > 0:  \n",
    "                try:\n",
    "                    similarity_matrix[i, j] = sim \n",
    "                except KeyError as e:\n",
    "                    print(f\"Error assigning similarity at index ({i}, {j}): {e}\")\n",
    "\n",
    "similarity_matrix.setdiag(1)\n",
    "# #Convert sparse matrices to a more efficient csr_matrix format. Lil_matrix is a linked list format that allows for quick and dynamic modification of the matrix structure.\n",
    "similarity_matrix = similarity_matrix.tocsr()\n",
    "\n",
    "#Save the sparse matrix as a file for future use, with diagonal values already set to 1\n",
    "save_npz('similar/cosine_similarity_matrix.npz', similarity_matrix)\n",
    "\n",
    "print(\"Similarity calculation completed, matrix saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Matrix shape: {similarity_matrix.shape}\")\n",
    "print(f\"Non-zero elements: {similarity_matrix.count_nonzero()}\")\n",
    "\n",
    "print(similarity_matrix[:10, :10].toarray())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an HG_Similarity_matrix, which is an association matrix containing hyperedge node information, to construct a problem hypergraph for the third channel\n",
    "HG_similarity_matrix = lil_matrix(similarity_matrix.shape)\n",
    "\n",
    "#Find elements greater than 0.8 (threshold) and set them to 1\n",
    "rows, cols = similarity_matrix.nonzero()\n",
    "for row, col in zip(rows, cols):\n",
    "    if similarity_matrix[row, col] > 0.8:\n",
    "        HG_similarity_matrix[row, col] = 1  #Set positions greater than 0.8 as 1\n",
    "\n",
    "#Convert the new matrix to csr_matrix format to improve efficiency\n",
    "HG_similarity_matrix = HG_similarity_matrix.tocsr()\n",
    "\n",
    "save_npz('similar/HG_cosine_similarity_matrix.npz', HG_similarity_matrix)\n",
    "\n",
    "print(\"Positions with values greater than 0.8 have been set to 1 and a new matrix has been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Matrix shape: {HG_similarity_matrix.shape}\")\n",
    "print(f\"Non-zero elements: {HG_similarity_matrix.count_nonzero()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HG_similarity_matrix = load_npz('similar/HG_cosine_similarity_matrix.npz')\n",
    "print(f\"Matrix shape: {HG_similarity_matrix.shape}\")\n",
    "\n",
    "Q_similarity_matrix = HG_similarity_matrix[:10, :10].todense()\n",
    "print(Q_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_adj(adj, is_symmetric=True):\n",
    "    \"\"\"Normalize adjacent matrix for GCN\"\"\"\n",
    "    if is_symmetric:\n",
    "        rowsum = np.array(adj.sum(1))\n",
    "        d_inv = np.power(rowsum + 1e-8, -1/2).flatten()\n",
    "        d_inv[np.isinf(d_inv)] = 0.\n",
    "        d_mat_inv = sp.diags(d_inv)\n",
    "        norm_adj = d_mat_inv * adj * d_mat_inv\n",
    "    else:\n",
    "        rowsum = np.array(adj.sum(1))\n",
    "        d_inv = np.power(rowsum + 1e-8, -1).flatten()\n",
    "        d_inv[np.isinf(d_inv)] = 0.\n",
    "        d_mat_inv = sp.diags(d_inv)\n",
    "        norm_adj = d_mat_inv * adj\n",
    "\n",
    "    return norm_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_csr_matrix_to_tensor(csr_matrix):\n",
    "    \"\"\"Transform csr matrix to tensor\"\"\"\n",
    "    coo = csr_matrix.tocoo()\n",
    "    values = coo.data\n",
    "    indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = coo.shape\n",
    "    sp_tensor = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "    return sp_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HG_question_similarity_graph_matrix = normalized_adj(adj=HG_similarity_matrix, is_symmetric=False)  #Normalized adjacency matrix\n",
    "HG_question_similarity_graph = transform_csr_matrix_to_tensor(HG_question_similarity_graph_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(HG_question_similarity_graph, 'similar/HG_question_similarity_graph.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sparse_H_user(sessions_dict, num_pois, num_users):\n",
    "    \"\"\"Generate sparse incidence matrix for hypergraph\"\"\"\n",
    "    H = np.zeros(shape=(num_pois, num_users))\n",
    "\n",
    "    for userID, sessions in sessions_dict.items():\n",
    "        for poi in sessions:\n",
    "            poi = int(poi) \n",
    "            userID = int(userID)\n",
    "            H[poi, userID] = 1\n",
    "    \n",
    "    H = sp.csr_matrix(H)\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skills_len)\n",
    "print(problems_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_skills = skills_len\n",
    "num_problems = problems_len\n",
    "keep_rate = 1\n",
    "H_cq = gen_sparse_H_user(problemid_skillid_key_value, num_skills, num_problems)  # [C, Q]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_matrix_drop_edge(csr_adj_matrix, keep_rate):\n",
    "    \"\"\"Drop edge on scipy.sparse.csr_matrix\"\"\"\n",
    "    if keep_rate == 1.0:\n",
    "        return csr_adj_matrix\n",
    "\n",
    "    coo = csr_adj_matrix.tocoo()\n",
    "    row = coo.row\n",
    "    col = coo.col\n",
    "    edgeNum = row.shape[0]\n",
    "\n",
    "    # generate edge mask\n",
    "    mask = np.floor(np.random.rand(edgeNum) + keep_rate).astype(np.bool_)\n",
    "\n",
    "    # get new values and indices\n",
    "    new_row = row[mask]\n",
    "    new_col = col[mask]\n",
    "    new_edgeNum = new_row.shape[0]\n",
    "    new_values = np.ones(new_edgeNum, dtype=np.float)\n",
    "\n",
    "    drop_adj_matrix = sp.csr_matrix((new_values, (new_row, new_col)), shape=coo.shape)\n",
    "\n",
    "    return drop_adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_cq = csr_matrix_drop_edge(H_cq, keep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyper_deg(incidence_matrix):\n",
    "    '''\n",
    "    # incidence_matrix = [num_nodes, num_hyperedges]\n",
    "    hyper_deg = np.array(incidence_matrix.sum(axis=axis)).squeeze()\n",
    "    hyper_deg[hyper_deg == 0.] = 1\n",
    "    hyper_deg = sp.diags(1.0 / hyper_deg)\n",
    "    '''\n",
    "\n",
    "    # H  = [num_node, num_edge]\n",
    "    # DV = [num_node, num_node]\n",
    "    # DV * H = [num_node, num_edge]\n",
    "\n",
    "    # HT = [num_edge, num_node]\n",
    "    # DE = [num_edge, num_edge]\n",
    "    # DE * HT = [num_edge, num_node]\n",
    "\n",
    "    # hyper_deg = incidence_matrix.sum(1)\n",
    "    # inv_hyper_deg = hyper_deg.power(-1)\n",
    "    # inv_hyper_deg_diag = sp.diags(inv_hyper_deg.toarray()[0])\n",
    "\n",
    "    rowsum = np.array(incidence_matrix.sum(1))\n",
    "    d_inv = np.power(rowsum, -1).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "    return d_mat_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypergraph convolution normalization operation\n",
    "Deg_H_cq = get_hyper_deg(H_cq)  \n",
    "HG_cq = Deg_H_cq * H_cq  \n",
    "HG_cq = transform_csr_matrix_to_tensor(HG_cq)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_qc = H_cq.T  \n",
    "Deg_H_qc = get_hyper_deg(H_qc)  \n",
    "HG_qc = Deg_H_qc * H_qc  \n",
    "HG_qc = transform_csr_matrix_to_tensor(HG_qc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(HG_cq, 'HG1/HG_cq.pt')\n",
    "torch.save(HG_qc, 'HG1/HG_qc.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Matrix shape: {HG_cq.shape}\")\n",
    "print(f\"Matrix shape: {HG_qc.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The second channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skills_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_concepts = skills_len\n",
    "cooccurrence_matrix = np.zeros((num_concepts, num_concepts), dtype=int)\n",
    "\n",
    "# #Example data\n",
    "data_gx = {\n",
    "    1: [40],\n",
    "    2: [50],\n",
    "    3: [32],\n",
    "    4: [32, 40, 50],\n",
    "    5: [40],\n",
    "    6: [50],\n",
    "    7: [32],\n",
    "    8: [32, 40, 50, 45],\n",
    "    9: [45],\n",
    "    10: [45]\n",
    "}\n",
    "\n",
    "for values in problemid_skillid_key_value.values():\n",
    "    # print(values)\n",
    "    if len(values) > 1:\n",
    "        \n",
    "        for i in range(len(values)):\n",
    "            for j in range(i + 1, len(values)):\n",
    "                val_i = values[i]\n",
    "                val_j = values[j]\n",
    "                \n",
    "                cooccurrence_matrix[int(val_i), int(val_j)] += 1\n",
    "                cooccurrence_matrix[int(val_j), int(val_i)] += 1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cooccurrence_matrix.shape)\n",
    "num_ones = np.sum(cooccurrence_matrix > 0)\n",
    "print(num_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On the basis of the original, it is necessary to add that the newly synthesized concept is connected to the sub concepts with edges, and if there is intersection between the sub concepts of the newly synthesized concept, there should also be edges\n",
    "#Inverse_mapping records which sub concepts make up a composite concept and generates a matrix\n",
    "#inverse_mapping：{0:[0],1:[1],...,112:[81,86],...}\n",
    "# #Update co-occurrence matrix\n",
    "for concept, sub_concepts in inverse_mapping.items():\n",
    "    # 1. The newly synthesized concept is connected to its sub concepts\n",
    "    for sub_concept in sub_concepts:\n",
    "        cooccurrence_matrix[concept, sub_concept] += 1\n",
    "        cooccurrence_matrix[sub_concept, concept] += 1\n",
    "\n",
    "    # 2. There are edges between newly synthesized concepts (if there are intersections between sub concepts)\n",
    "    for other_concept, other_sub_concepts in inverse_mapping.items():\n",
    "        if concept != other_concept:  \n",
    "            if set(sub_concepts) & set(other_sub_concepts):  \n",
    "                cooccurrence_matrix[concept, other_concept] += 1\n",
    "                cooccurrence_matrix[other_concept, concept] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ones = np.sum(cooccurrence_matrix > 0)\n",
    "print(num_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('concept_cooccurrence/concept_cooccurrence_matrix.npy', cooccurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_cooccurrence_matrix = np.load('concept_cooccurrence/concept_cooccurrence_matrix.npy')\n",
    "print(\"Matrix loaded:\")\n",
    "print(concept_cooccurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Matrix shape: {concept_cooccurrence_matrix.shape}\")\n",
    "print(f\"Non-zero elements: {np.count_nonzero(concept_cooccurrence_matrix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_indices = np.nonzero(concept_cooccurrence_matrix)\n",
    "\n",
    "rows, cols = non_zero_indices\n",
    "for r, c in zip(rows, cols):\n",
    "    value = concept_cooccurrence_matrix[r, c]\n",
    "    print(f\"Non zero elements: row {r}, Column {c}, Value {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide each value of the co-occurrence matrix by the total number of concepts, and use it as the weight matrix for concept co-occurrence: W_comocurnence_matrix to prevent future improvements\n",
    "divisor = skills_len\n",
    "W_cooccurrence_matrix = np.round(cooccurrence_matrix / divisor, 3)\n",
    "np.save('concept_cooccurrence/concept_W_cooccurrence_matrix.npy', W_cooccurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W_cooccurrence_matrix = np.load('concept_cooccurrence/concept_W_cooccurrence_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_cooccurrence_adjacent_matrix = np.load('concept_cooccurrence/concept_cooccurrence_matrix.npy')\n",
    "print(\"Matrix loaded:\")\n",
    "print(concept_cooccurrence_adjacent_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_cooccurrence_adjacent_matrix[concept_cooccurrence_matrix >= 1] = 1\n",
    "np.save('concept_cooccurrence/concept_cooccurrence_adjacent_matrix.npy', concept_cooccurrence_adjacent_matrix)\n",
    "\n",
    "print(\"Matrix modification completed and saved as: concept_cooccurrence_adjacent_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Matrix shape: {concept_cooccurrence_adjacent_matrix.shape}\")\n",
    "print(f\"Non-zero elements: {np.count_nonzero(concept_cooccurrence_adjacent_matrix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_indices = np.nonzero(concept_cooccurrence_adjacent_matrix)\n",
    "\n",
    "rows, cols = non_zero_indices\n",
    "for r, c in zip(rows, cols):\n",
    "    value = concept_cooccurrence_adjacent_matrix[r, c]\n",
    "    print(f\"Non zero elements: row {r}, Column {c}, Value {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 10  \n",
    "cols = 10  \n",
    "sub_matrix = concept_cooccurrence_adjacent_matrix[:rows, :cols]\n",
    "print(\"矩阵的前 {} 行和前 {} 列：\".format(rows, cols))\n",
    "print(sub_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as GCN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the concept co-occurrence adjacency matrix. The diagonals of this adjacency matrix are all 0, and PYG's GCN will automatically help fill in 1\n",
    "concept_cooccurrence_adjacent_matrix = np.load('concept_cooccurrence/concept_cooccurrence_adjacent_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(sum(concept_cooccurrence_adjacent_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_cooccurrence_adjacent_matrix = torch.tensor(concept_cooccurrence_adjacent_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the index of non-zero terms in the adjacency matrix and generate edge_index\n",
    "edge_index = concept_cooccurrence_adjacent_matrix.nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(edge_index, 'concept_cooccurrence/edge_index.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
